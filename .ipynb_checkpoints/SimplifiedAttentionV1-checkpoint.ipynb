{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae52e803-1806-4066-9d21-0184dbc6b911",
   "metadata": {},
   "source": [
    "# Task : Implement Attention mechanism without training weights\n",
    "### Todo:\n",
    "- Calculate the context vectors for a mock input\n",
    "- step1: Calculate attention scores\n",
    "- step2: Normalise the attention scores using softmax\n",
    "- step3: Calculate context vectors. \n",
    "- ***Lets start by taking a mock text input***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf9abbe-d34e-452f-a26d-8f8c035ae6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5168cc90-a64d-4ce0-81dd-83981cd6fc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c2260-994d-4f21-874b-8d0e4cfd554b",
   "metadata": {},
   "source": [
    "- Lets calculate attention score for a single input embedding \n",
    "-  To do that we will have to find out the dot prodcut between query and every vector in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0d4944-79bb-4ace-bdad-620ff1c605c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention score for our query x^2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "attention_scores1 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores1[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(f\" Attention score for our query x^2: {attention_scores1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485b271-35ae-4882-a2de-f1c676dc4347",
   "metadata": {},
   "source": [
    "- Now lets normalize these scores with softmax \n",
    "- Normalizing helps in esier interpretation of attenion weights, also using it will keep it consistent with other weights during model taining\n",
    "- Using pytorch implementaion of softmax also helps with navigating overflow issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bdda75c-dce8-4a30-bc9f-a7ba93c09d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for our query x^2: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Attention weights sum 1.0\n"
     ]
    }
   ],
   "source": [
    " \n",
    "attention_weights1 = torch.softmax(attention_scores1, dim=0)\n",
    "print(f\"Attention weights for our query x^2: {attention_weights1}\")\n",
    "print(f\"Attention weights sum {attention_weights1.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f45f65-9d21-4744-a8d8-8034f9087e9e",
   "metadata": {},
   "source": [
    "- Now lets get the context vector for our query. \n",
    "- To do that We will have to multiply each attention weight of query x^2 ([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "- with each of the input embedding vecotrs. This will scale the embedding vectors down by the factor of attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7821101b-a02a-4782-a977-2e8d02314be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for our query x^2: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "context_vec1 = torch.empty(query.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec1 += attention_weights1[i] * x_i\n",
    "print(f\"Context vector for our query x^2: {context_vec1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d1572-d474-40d2-83e8-270935b3a456",
   "metadata": {},
   "source": [
    "- Now that we have understood the steps to calculate the context vector, we can do this for all the input embeddings\n",
    "- lets first create attention scores for our entire input. Using for loops is slow, so lets use th matrix multiplication.\n",
    "- all we have to do is multiply inputs with the transpose of inputs\n",
    "- we will see that we get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ee066e-6a71-4248-87f3-9f233a2f5390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e92fb5-a26a-498e-bef4-fea263ea3ba8",
   "metadata": {},
   "source": [
    "***Notice how the scores for the token 'Journey' are exactly the same as when we calculated them using for loop***\n",
    "- Now lets normalize the scores using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76fce848-87ab-4394-8bed-c5945a2e96ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0c110-3541-4c5a-93d7-f2a98cd3b886",
   "metadata": {},
   "source": [
    "- Now we need to calculate context vectors for each of our input embeddings.\n",
    "- We could multiply attention_weights(6x6) with the input embeddings(6x3) to get the context vecotrs for each embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "591a5ea4-a73d-445c-8908-ed2869a706be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_weights @ inputs \n",
    "print(context_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
