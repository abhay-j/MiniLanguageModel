{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3accf48b-35ec-4a1f-b820-8c9297948ae3",
   "metadata": {},
   "source": [
    "### large language models are just neural networks that need an input to produce an output. \n",
    "- But we cannot give natural language as an input to these networks so we have to figure out a way to break text into chunks and then convert these chunks into some sort of numbers\n",
    "- Breaking the input text to the language model into smaller chunks called 'tokens' is called tokenization.\n",
    "- When we tokenize a 'document' (single unit of input to a language model) we endup with tokens.\n",
    "- These tokens are further encoded so that we endup with 'Token ids'\n",
    "- We can use these token ids to create embeddings that will be given to a language model as input.\n",
    "- In this notebook we will build simple tokenizer to tokenize the \"The Prophet\" by Khalil Gibran.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaab3b0-ccb9-40d4-ba11-9fb17fe51098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86102"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dprpht.txt','r',encoding = 'utf-8') as f:\n",
    "    raw_data = f.read()\n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e837193-c8c1-4610-bbaa-284691a03679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Prophet', '\\n', '', ' ', '', ' ', '', ' ', '', ' ', '', '\\n', 'This', ' ', 'ebook', ' ', 'is', ' ', 'for', ' ', 'the', ' ', 'use', ' ', 'of', ' ', 'anyone', ' ', 'anywhere', ' ', 'in', ' ', 'the', ' ', 'United', ' ', 'States', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "split_space = re.split(r'(\\s)', raw_data)\n",
    "print(split_space[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849108c-e9b0-48ef-a101-26f61d29b43d",
   "metadata": {},
   "source": [
    "- We can see there are a bunch of special characters we might need to take into consideration.\n",
    "- There are also , illustrations in the book, marked using \"\\[Illustration: ####]\" word. we will replace it with \"\\<ILLUSTRATION>\"\n",
    "- We will be also be stripping away the beginnign and the end of the document makeed by *** START OF THE PROJECT GUTENBERG EBOOK THE PROPHET *** and *** END OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\n",
    "- Also, lets just put everything into a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5fe365-bd2f-4f8a-8632-4d3d6c44137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_data):\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "\n",
    "    start_idx = raw_data.find(start_marker)\n",
    "    end_idx = raw_data.find(end_marker)\n",
    "\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        raise ValueError(\"Start or end index not found\")\n",
    "   \n",
    "    # Slice the content between markers\n",
    "    content = raw_data[start_idx + len(start_marker):end_idx]\n",
    "\n",
    "    content = content.replace('\\n', \" \")\n",
    "    content = re.sub(r'\\[Illustration:\\s*\\d{4}\\]', ' <ILLUSTRATION> ', content)\n",
    "    preprocessed = re.split(r'(\\s+|[.,:;?!“”\"()\\'’\\-_—*[\\]])', content)\n",
    "    preprocessed = [item for item in preprocessed if item.strip()]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8683d1f1-1e12-4940-9996-0272cc4883c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "# end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "# start_idx = raw_data.find(start_marker)\n",
    "# end_idx = raw_data.find(end_marker)\n",
    "# print(start_idx, end_idx)\n",
    "preprocessed = tokenize(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c753e47-eb4d-4a5b-9606-76981fe76d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'PROPHET', 'By', 'Kahlil', 'Gibran', 'New', 'York', ':', 'Alfred', 'A', '.', 'Knopf', '1923', '_', 'The', 'Twelve', 'Illustrations', 'In', 'This', 'Volume', 'Are', 'Reproduced', 'From', 'Original', 'Drawings', 'By', 'The', 'Author', '_', '“', 'His', 'power', 'came', 'from', 'some', 'great', 'reservoir', 'of', 'spiritual', 'life', 'else', 'it', 'could', 'not', 'have', 'been', 'so', 'universal', 'and', 'so']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047024f-9352-4150-873a-701ac1f99327",
   "metadata": {},
   "source": [
    "### Now we have the list of our preprocessed tokens. For the next step we will be creating token ids and vocabulary.\n",
    "- We can get token ids by first sorting the 'preprocesssed' list and getting the indices of all the unique items in it.\n",
    "- Then we can create a vocabulary. Vocabulary is nothing but a dictonary that maps tokens to its ids. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49b05cb-2724-431d-8d31-e9a28b661582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in vocab: 2162\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"number of words in vocab: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d99a281-ea7d-4a53-be5f-37743da39a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f7f556-ce4a-47bc-b50f-808800f528fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('*', 0)\n",
      "(',', 1)\n",
      "('-', 2)\n",
      "('.', 3)\n",
      "('1918', 4)\n",
      "('1919', 5)\n",
      "('1920', 6)\n",
      "('1923', 7)\n",
      "('1926', 8)\n",
      "('1928', 9)\n",
      "('1931', 10)\n",
      "('1932', 11)\n",
      "('1933', 12)\n",
      "('1934', 13)\n",
      "('1948', 14)\n",
      "(':', 15)\n",
      "(';', 16)\n",
      "('<ILLUSTRATION>', 17)\n",
      "('?', 18)\n",
      "('A', 19)\n",
      "('After', 20)\n",
      "('Alfred', 21)\n",
      "('All', 22)\n",
      "('Almitra', 23)\n",
      "('Almustafa', 24)\n",
      "('Alone', 25)\n",
      "('Always', 26)\n",
      "('Am', 27)\n",
      "('Among', 28)\n",
      "('And', 29)\n",
      "('Archer', 30)\n",
      "('Are', 31)\n",
      "('At', 32)\n",
      "('Author', 33)\n",
      "('Ay', 34)\n",
      "('Aye', 35)\n",
      "('BOOKS', 36)\n",
      "('Be', 37)\n",
      "('Beauty', 38)\n",
      "('Blessed', 39)\n",
      "('Bragdon', 40)\n",
      "('Brief', 41)\n",
      "('Build', 42)\n",
      "('But', 43)\n",
      "('Buying', 44)\n",
      "('By', 45)\n",
      "('CONTENTS', 46)\n",
      "('Children', 47)\n",
      "('Claude', 48)\n",
      "('Clothes', 49)\n",
      "('Come', 50)\n",
      "('Coming', 51)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c3f3f-ccf0-4f32-845c-83d738a6c2ce",
   "metadata": {},
   "source": [
    "# Great! Now we have a vocabulary of all the words present in our document. \n",
    "- However, there might be instances where we might encounter an input word that might not be in our vocabulary.\n",
    "- To handle such a case we will extend our current vocabulary to include a \"<|unk|>\". This special token will correspond to a word that is not in our vocabulary.\n",
    "- We will also add another token \"<|endoftext|>\" to mark end of our data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d654c4-c092-4cc5-8216-1f889436353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])                   \n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614fc531-82d1-43f4-a5ce-31d445e5487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d59fabd-7588-4ec4-9c03-1fb581345ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('’', 2159)\n",
      "('“', 2160)\n",
      "('”', 2161)\n",
      "('<|endoftext|>', 2162)\n",
      "('<|unk|>', 2163)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae86d8b-f2b1-4774-b711-1771d818e801",
   "metadata": {},
   "source": [
    "# Now, we also know how to deal with the words that are not in our vocabulary!\n",
    "- Further down the line, we will need a mechanism to convert the token ids back into the original text.\n",
    "- Lets, write a **SimpleTokenizer** Class, which will have 2 methods.\n",
    "- - Encode\n",
    "  - Decode\n",
    "- **encode** method will take the input text, perform the preprocessing and return the token ids.\n",
    "- **decode** method will take the token ids and return the corresponding text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5731518b-ccb5-4ae2-b17b-c7d7c2e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:t for t,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'(\\s+|[.,:;?!“”\"()\\'’\\-_—*[\\]])', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [ item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [ self.str_to_int[item] for item in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'(\\s+|[.,:;?!“”\"()\\'’\\-_—*[\\]])', r'\\1', text)\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0737cbf-de0a-453b-8628-8b56c82f47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 1843, 2029, 1, 2163, 1060, 1667, 1, 1191, 1893, 1843, 2163, 1, 2074, 1843, 2163, 1161, 1333, 1843, 498, 2163, 1162, 660, 1240, 3]\n"
     ]
    }
   ],
   "source": [
    "simpletokenizer = SimpleTokenizer(vocab)\n",
    "sample_text = \"And the wanderer, wrapped in silence, looked to the horizon, where the neon lights of the city flickered like distant memories.\"\n",
    "token_ids = simpletokenizer.encode(sample_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523625ac-c531-426f-b24a-cf83de902c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the wanderer , <|unk|> in silence , looked to the <|unk|> , where the <|unk|> lights of the city <|unk|> like distant memories .\n"
     ]
    }
   ],
   "source": [
    "decoded_text = simpletokenizer.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd86046-f5c1-49d0-96df-f22800092338",
   "metadata": {},
   "source": [
    "### This is a simplified implementation of the tokenizer, however large language models like GPT use something called Byte-Pair-Encoding to break a word down even  further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26700aed-0fa1-4d97-9141-31e83c6e724c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
