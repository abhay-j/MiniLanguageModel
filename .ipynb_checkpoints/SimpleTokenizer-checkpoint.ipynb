{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3accf48b-35ec-4a1f-b820-8c9297948ae3",
   "metadata": {},
   "source": [
    "### large language models are just neural networks that need an input to produce an output. \n",
    "- But we cannot give natural language as an input to these networks so we have to figure out a way to break text into chunks and then convert these chunks into some sort of numbers\n",
    "- Breaking the input text to the language model into smaller chunks called 'tokens' is called tokenization.\n",
    "- When we tokenize a 'document' (single unit of input to a language model) we endup with tokens.\n",
    "- These tokens are further encoded so that we endup with 'Token ids'\n",
    "- We can use these token ids to create embeddings that will be given to a language model as input.\n",
    "- In this notebook we will build simple tokenizer to tokenize the \"The Prophet\" by Khalil Ghibran.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dc6f79-35eb-4dc9-b250-51be44bf26fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SimpleTokenizer.ipynb',\n",
       " 'README.md',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'dprpht.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c16d0f72-f5e5-4bf3-99c3-9de16b1be54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the raw_data string: 86102\n",
      "﻿The Project Gutenberg eBook of The Prophet\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in th\n"
     ]
    }
   ],
   "source": [
    "with open('dprpht.txt', 'r', encoding = \"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "print(f\"Total characters in the raw_data string: {len(raw_data)}\")\n",
    "print(raw_data[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa96559-ac47-4a61-bc5f-1227fea871e8",
   "metadata": {},
   "source": [
    "- This is a look at the first 100 characters of the text.\n",
    "- Next, we will try to break it down into its constituents.\n",
    "- First, lets start by breaking the raw_text down by spaces.\n",
    "- we will use Python's 're' regular expression library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661d8adb-0cae-4862-b417-ca5538b5a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Prophet', '\\n', '', ' ', '', ' ', '', ' ', '', ' ', '', '\\n', 'This', ' ', 'ebook', ' ', 'is', ' ', 'for', ' ', 'the', ' ', 'use', ' ', 'of', ' ', 'anyone', ' ', 'anywhere', ' ', 'in', ' ', 'the', ' ', 'United', ' ', 'States', ' ', 'and', '\\n', 'most', ' ', 'other', ' ', 'parts', ' ', 'of', ' ', 'the', ' ', 'world', ' ', 'at', ' ', 'no', ' ', 'cost', ' ', 'and', ' ', 'with', ' ', 'almost', ' ', 'no', ' ', 'restrictions', '\\n', 'whatsoever.', ' ', 'You', ' ', 'may', ' ', 'copy', ' ', 'it,', ' ', 'give', ' ', 'it', ' ', 'away', ' ', 'or', ' ', 're-use', ' ', 'it', ' ', 'under', ' ', 'the', ' ', 'terms', '\\n', 'of', ' ', 'the', ' ', 'Project', ' ', 'Gutenberg', ' ', 'License', ' ', 'included', ' ', 'with', ' ', 'this', ' ', 'ebook', ' ', 'or', ' ', 'online', '\\n', 'at', ' ', 'www.gutenberg.org.', ' ', 'If', ' ', 'you', ' ', 'are', ' ', 'not', ' ', 'located', ' ', 'in', ' ', 'the', ' ', 'United', ' ', 'States,', '\\n', 'you', ' ', 'will', ' ', 'have', ' ', 'to', ' ', 'check', ' ', 'the', ' ', 'laws', ' ', 'of', ' ', 'the', ' ', 'country', ' ', 'where', ' ', 'you', ' ', 'are', ' ', 'located', '\\n', 'before', ' ', 'using', ' ', 'this', ' ', 'eBook.', '\\n', '', '\\n', 'Title:', ' ', 'The', ' ', 'Prophet', '\\n', '', '\\n', 'Author:', ' ', 'Kahlil', ' ', 'Gibran', '\\n', '', '\\n', 'Release', ' ', 'date:', ' ', 'January', ' ', '1,', ' ', '2019', ' ', '[eBook', ' ', '#58585]', '\\n', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', '', ' ', 'Most', ' ', 'recently', ' ', 'updated:', ' ', 'March', ' ', '13,', ' ', '2025', '\\n', '', '\\n', 'Language:', ' ', 'English', '\\n', '', '\\n', 'Credits:', ' ', 'David', ' ', 'Widger', '\\n', '', '\\n', '', '\\n', '***', ' ', 'START', ' ', 'OF', ' ', 'THE', ' ', 'PROJECT', ' ', 'GUTENBERG', ' ', 'EBOOK', ' ', 'THE', ' ', 'PROPHET', ' ', '***', '\\n', '', '\\n', '', '\\n', '', '\\n', '', '\\n', 'THE', ' ', 'PROPHET', '\\n', '', '\\n', 'By', ' ', 'Kahlil', ' ', 'Gibran', '\\n', '', '\\n', 'New', ' ', 'York:', ' ', 'Alfred', ' ', 'A.', ' ', 'Knopf', '\\n', '', '\\n', '1923', '\\n', '', '\\n', '_The', ' ', 'Twelve', ' ', 'Illustrations', ' ', 'In', ' ', 'This', ' ', 'Volume', '\\n', 'Are', ' ', 'Reproduced', ' ', 'From', ' ', 'Original', ' ', 'Drawings', ' ', 'By', '\\n', 'The', ' ', 'Author_', '\\n', '', '\\n', '', '\\n', '', '\\n', '', '\\n', '“His', ' ', 'power', ' ', 'came', ' ', 'from', ' ', 'some', ' ', 'great', ' ', 'reservoir', '\\n', 'of', ' ', 'spiritual', ' ', 'life', ' ', 'else', ' ', 'it', ' ', 'could', ' ', 'not', ' ', 'have', '\\n', 'been', ' ', 'so', ' ', 'universal', ' ', 'and', ' ', 'so', ' ', 'potent,', ' ', 'but', ' ', 'the', '\\n', 'majesty', ' ', 'and', ' ', 'beauty', ' ', 'of', ' ', 'the', ' ', 'language', ' ', 'with', '\\n', 'which', ' ', 'he', ' ', 'clothed', ' ', 'it', ' ', 'were', ' ', 'all', ' ', 'his', ' ', 'own?”', '\\n', '', '\\n', '--Claude', ' ', 'Bragdon', '\\n', '', '\\n', '', '\\n', 'THE', ' ', 'BOOKS', ' ', 'OF', ' ', 'KAHLIL', ' ', 'GIBRAN', '\\n', '', '\\n', 'The', ' ', 'Madman.', ' ', '1918', ' ', 'Twenty', ' ', 'Drawings.', ' ', '1919', '\\n', 'The', ' ', 'Forerunner.', ' ', '1920', ' ', 'The', ' ', 'Prophet.', ' ', '1923', '\\n', 'Sand', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "preprocessed = re.split(r'(\\s)', raw_data)\n",
    "print(preprocessed[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849108c-e9b0-48ef-a101-26f61d29b43d",
   "metadata": {},
   "source": [
    "- We can see there are a bunch of special characters we might need to take into consideration.\n",
    "- There are also , illustrations in the book, marked using \"\\[Illustration: ####]\" word. we will replace it with \"\\<ILLUSTRATION>\"\n",
    "- We will be also be stripping away the beginnign and the end of the document makeed by *** START OF THE PROJECT GUTENBERG EBOOK THE PROPHET *** and *** END OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\n",
    "- Also, lets just put everything into a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5fe365-bd2f-4f8a-8632-4d3d6c44137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_data):\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK THE PROPHET ***\"\n",
    "\n",
    "    start_idx = raw_data.find(start_marker)\n",
    "    end_idx = raw_data.find(end_marker)\n",
    "\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        raise ValueError(\"Start or end index not found\")\n",
    "   \n",
    "    # Slice the content between markers\n",
    "    content = text[start_idx + len(start_marker):end_idx]\n",
    "\n",
    "    content = content.replace('\\n', \" \")\n",
    "    text = re.sub(r'\\[Illustration:\\s*\\d{4}\\]', ' <ILLUSTRATION> ', raw_data)\n",
    "    preprocessed = re.split(r'(\\s+|[.,:;?!“”\"()\\'’\\-_—*[\\]])', text)\n",
    "    preprocessed = [item for item in preprocessed if item.strip()]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b50c6-3145-47b8-8266-728d5e537a2a",
   "metadata": {},
   "source": [
    "- Now lets remove the spaces from the preprocessed list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8683d1f1-1e12-4940-9996-0272cc4883c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Start or end index not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m preprocessed = \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(preprocesses[:\u001b[32m50\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(raw_data)\u001b[39m\n\u001b[32m      6\u001b[39m end_idx = raw_data.find(end_marker)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_idx == -\u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m end_idx == -\u001b[32m1\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mStart or end index not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Slice the content between markers\u001b[39;00m\n\u001b[32m     12\u001b[39m content = text[start_idx + \u001b[38;5;28mlen\u001b[39m(start_marker):end_idx]\n",
      "\u001b[31mValueError\u001b[39m: Start or end index not found"
     ]
    }
   ],
   "source": [
    "preprocessed = tokenize(raw_data)\n",
    "print(preprocesses[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dc100-eea9-407b-a978-244c3491278b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
