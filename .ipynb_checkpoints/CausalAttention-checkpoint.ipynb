{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f82062-2af2-48aa-9902-450bb83a7a46",
   "metadata": {},
   "source": [
    "# Causal Attention with Droupouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71310ed6-88cb-43aa-870d-7e87293dfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bf85917-2873-4880-8f54-c3b5d385499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30422d39-2413-472a-b590-38093284f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18269f61-3f75-4393-99d4-0a7d4815fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sav2 = SelfAttentionV2(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eff54ff7-4e94-494d-9a88-87ccdc57c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2327,  0.1055,  0.1098,  0.0913,  0.1549,  0.0521],\n",
      "        [-0.2396,  0.1015,  0.1057,  0.0902,  0.1501,  0.0518],\n",
      "        [-0.2323,  0.1004,  0.1045,  0.0885,  0.1481,  0.0507],\n",
      "        [-0.1344,  0.0502,  0.0523,  0.0470,  0.0753,  0.0272],\n",
      "        [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0174],\n",
      "        [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "keys = sav2.W_key(inputs)\n",
    "queries = sav2.W_query(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54c5b1-cc02-479b-a591-7b55e342d896",
   "metadata": {},
   "source": [
    "### Here, we are trying to mask out attention scores above the diagonal such that, our model cannot cheat, that is\n",
    "### it only has access to tokens till i-1 if it is trying to predict ith token\n",
    "### we can achive this my simply multiplying our attention scores with a lower-triangular matrix of ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af9160c0-fdad-4712-9759-53e4e16c15fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = 6\n",
    "simple_mask = torch.tril(torch.ones(context_len, context_len))\n",
    "print(simple_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee6e9871-201f-4524-9e00-6bb44cf122a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2327,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2396,  0.1015,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2323,  0.1004,  0.1045,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1344,  0.0502,  0.0523,  0.0470,  0.0000,  0.0000],\n",
      "        [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,  0.0000],\n",
      "        [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "simple_masked_scores = attention_scores * simple_mask\n",
    "print(simple_masked_scores)\n",
    "#However, the data till the ith token is still influenced by the i+1th token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "215e84b2-6b05-4eb6-9ba9-6ef5e1653439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 1.7352, -0.7352, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 8.4746, -3.6631, -3.8116, -0.0000, -0.0000, -0.0000],\n",
      "        [-8.9549,  3.3429,  3.4830,  3.1290,  0.0000,  0.0000],\n",
      "        [-0.1997,  0.2974,  0.3077,  0.1896,  0.4050,  0.0000],\n",
      "        [-1.7071,  0.5185,  0.5414,  0.5327,  0.7999,  0.3146]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = simple_masked_scores.sum(dim = 1, keepdim = True)\n",
    "simple_masked_weights = simple_masked_scores / row_sums\n",
    "print(simple_masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d21a37-b25f-4407-b1f2-37ef4ac9e059",
   "metadata": {},
   "source": [
    "### We successfully masked out the weights below diagonal. However, our current weights still are influenced by our future weights (masked out). This is called data-leakage\n",
    "### We can take use of the softmax function to handle this. \n",
    "### We will create an upper-triangular matrix of ones and replace all the ones with -inf. While applying softmax, -inf will be considered as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9b9686-361f-459f-8a6b-330b4da4ef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4772,  0.1063],\n",
       "        [ 0.3303, -0.1816],\n",
       "        [-1.0134, -2.8082],\n",
       "        [ 1.4447,  3.3911],\n",
       "        [ 0.5725,  0.4566],\n",
       "        [ 0.5892,  0.8548]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = sav2.W_value(inputs)\n",
    "simple_masked_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d2a9ca1-374c-4601-b864-293f5eaf57b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(context_len, context_len)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc0066e5-0b6f-4db5-aea5-6205f2bc9617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(ones, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f87dcad1-34c4-4586-a7a4-7632cebb15bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],\n",
      "        [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],\n",
      "        [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],\n",
      "        [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42bf304c-30dd-4758-b88f-be0eed98e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n",
      "        [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n",
      "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88632978-f78a-43be-8baf-a336811a4f55",
   "metadata": {},
   "source": [
    "### Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero.\n",
    "### The softmax function then recalculates attention weights only among the unmasked tokens.\n",
    "### This process ensures no information leakage from masked tokens, focusing the model solely on the intended data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fe891-c4fe-4966-9f37-837ae7457fc6",
   "metadata": {},
   "source": [
    "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT\n",
    "- When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero.\n",
    "- To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2.\n",
    "- This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d902daf9-6b03-471a-a71b-83e1741fe3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5659, 0.7160, 0.7181, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5159, 0.5167, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4047, 0.0000, 0.3993, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3430, 0.3437, 0.3434, 0.3516, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6d728-9474-4cc0-8940-dd4a3c8f1446",
   "metadata": {},
   "source": [
    "### Creating Causal Attention Class with Dropouts class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e219544-cf08-44e3-95e6-d2052c26582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afd69981-951a-4126-b5d0-8a9672cbe36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e99aa9e5-11bc-4f68-9c11-059ba93b7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, _ = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1,2) \n",
    "        masked_scores = attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(masked_scores/keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c121d262-9e3a-448f-91ab-42c352dae08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "dim_in = 3\n",
    "dim_out = 2\n",
    "ca = CausalAttention(dim_in, dim_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1d60bff-18a2-40a3-ab35-7291606de919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa500b6-6dec-4806-9d1b-68ca7adf1ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
