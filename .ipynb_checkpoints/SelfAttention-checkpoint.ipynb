{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7324c85-f566-4e49-b3d0-f1d5f3d77d8b",
   "metadata": {},
   "source": [
    "# Self-attention with trainable  Query, Key, Value weights\n",
    "- Todo:\n",
    "- create query, key, value weight matrices to transofrm the input embeddings.\n",
    "- We need these weight metrices because without these the relationship between tokens like \"journey\" and \"starts\" would indeed be fixed across all contexts. We'd be limited to using only the original embedding space to determine attention\n",
    "- We multiply the input embeddings with the query, key, value weight matrices\n",
    "- we derive the attention scores by multiplying the query and the key matrix.\n",
    "- These attention scores are normalised by applying softmax to derive attention weights.\n",
    "- The attention weight matrix is multiplied by the value matrix to derive our context vecots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757debdd-63ad-4beb-9c93-0a24708bced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],# Your (x^1)\n",
    "   [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "   [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]]  # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db1ad89-26d7-41bc-a13f-e6e70bf63bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d7b323-2eca-4374-b6b1-75b609bd35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the weight matrices we will have to use the shape : inputs.shape[1] x no. of o/p dims we want to have\n",
    "#Note that in GPT-like models, the input and output dimensions are usually the same.\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b260c5e-b83f-480e-a5ae-d2b56ff1659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfeeed-5faf-4f77-84af-16e8739bc23b",
   "metadata": {},
   "source": [
    "- Tehse are the query, key, value weight matrices initialised with random values, these will be trained.\n",
    "- Note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes.\n",
    "- If we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad8d634-909d-4023-9f3f-90adc959f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) tensor([0.4433, 1.1419])\n"
     ]
    }
   ],
   "source": [
    "# computing the query, key, and value matrices for the input embedding \n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2, key_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe50fa1-b23b-4e15-bacc-151d7ea8347c",
   "metadata": {},
   "source": [
    "- Even though our temporary goal is to only compute the one context vector z(2), we still require the key and value vectors for all input elements.\n",
    "- This is because they are involved in computing the attention weights with respect to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26e4e45-a4f1-4233-aa0a-1d8a72ce73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys shape :torch.Size([6, 2]), values shape :torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print (f\"keys shape :{keys.shape}, values shape :{values.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78fe136-ba49-4fd3-8d63-d3ea6a752a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# attention score of query_2 with respect to only keys[1]\n",
    "keys_2 = keys[1]\n",
    "attention_score_query_2 = query_2.dot(keys_2)\n",
    "print(attention_score_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40b176f-a977-47d7-b937-6cf78716da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# lets find out attention score for  query_2 with respect to all keys\n",
    "attention_scores_query_2 = query_2 @ keys.T\n",
    "print(attention_scores_query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08858bf0-645c-4b69-8ae2-c0b114252804",
   "metadata": {},
   "source": [
    "- We compute the attention weights by scaling the attention scores and using the softmax function we used earlier.\n",
    "- The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af20e8c7-f306-4fb5-929b-bba513a4282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights for q2: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "dim_k = keys.shape[-1]\n",
    "attention_weights_q2 = torch.softmax((attention_scores_query_2 /dim_k ** 0.5) , dim = -1)\n",
    "print(f\"attention weights for q2: {attention_weights_q2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ad17b-abe7-4327-9897-faf27fca32ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538739f4-a1ad-4b61-8fd2-aa492b808f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
